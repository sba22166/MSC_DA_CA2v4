{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b1cefd",
   "metadata": {},
   "source": [
    "# 1.0 Spark & Mongo Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc554e2a",
   "metadata": {},
   "source": [
    "## 1.1 Assessment Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ac57a",
   "metadata": {},
   "source": [
    "This notebook will follow this procedure\n",
    "\n",
    "- Read in Data from Hadoop/HFDS\n",
    "\n",
    "- Create Spark Session and read in the csv's\n",
    "\n",
    "- Perform some spark processing\n",
    "\n",
    "- Store SOurce Dataset in NoSQL DB - MOngoDB chosen\n",
    "\n",
    "- Extract dataset from MongDB to csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7ec577",
   "metadata": {},
   "source": [
    "## 1.2 Reading from Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5151f7c",
   "metadata": {},
   "source": [
    "This section involves reading in our twitter data which is stored in hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa629d7",
   "metadata": {},
   "source": [
    "My data is stored in hdfs as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d72b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hduser@Vm:~/$ hadoop fs -ls /\n",
    "#Found 2 items\n",
    "#-rw-r--r--   1 hduser supergroup 1813056825 2023-05-23 22:11 /tweets.csv\n",
    "#-rw-r--r--   1 hduser supergroup   85123366 2023-05-24 10:39 /users.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d1ee69",
   "metadata": {},
   "source": [
    "## 1.3 Spark Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08a76dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#importing spark session \n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7562f9a",
   "metadata": {},
   "source": [
    "Creating a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "774005a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a551e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mimising the error output for spark processes\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed3662b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that my data is stored in hdfs, lets read it into spark\n",
    "#Data is stored in / , which is hdfs directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de48e6a9",
   "metadata": {},
   "source": [
    "On first attempt at reading in my data, the data was all of type string.\n",
    "\n",
    "Now lets create a shcema that we can use going forward first\n",
    "\n",
    "This dataset came with 2 csv files, one for tweets, and one for users/user details. Later I will merge them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e44cd706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+------------+--------------------+----------+--------------------+--------------------+--------------+-----------+--------------+\n",
      "|            tweetUrl|                date|     renderedContent|             tweetId|              userId|replyCount|retweetCount|           likeCount|quoteCount|              source|               media|retweetedTweet|quotedTweet|mentionedUsers|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+------------+--------------------+----------+--------------------+--------------------+--------------+-----------+--------------+\n",
      "|https://twitter.c...|2021-03-30 03:33:...|          Support 👇|                null|                null|      null|        null|                null|      null|                null|                null|          null|       null|          null|\n",
      "|    #FarmersProtest\"|1.376739399593910...|1.015969769760096...|                   0|                   0|         0|           0|\"<a href=\"\"http:/...|      null|                null|                null|          null|       null|          null|\n",
      "|https://twitter.c...|2021-03-30 03:33:...|Supporting farmer...|1.376739306287427...|1.332937272581263...|         0|           0|                   0|         0|\"<a href=\"\"http:/...|[{'previewUrl': '...|          null|       null|          null|\n",
      "|https://twitter.c...|2021-03-30 03:31:...|Support farmers i...|1.376738704128020...|1.332937272581263...|         0|           0|                   0|         0|\"<a href=\"\"http:/...|[{'previewUrl': '...|          null|       null|          null|\n",
      "|https://twitter.c...|2021-03-30 03:30:...|#StopHateAgainstF...|1.376738640542400...|1.308356658582618...|         0|           1|                   3|         0|\"<a href=\"\"http:/...|                null|          null|       null|          null|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+------------+--------------------+----------+--------------------+--------------------+--------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+------------------+--------+--------------------+---------------+--------+--------------------+--------------+------------+-------------+--------------------+-----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|            username|       displayname|  userId|      rawDescription|descriptionUrls|verified|             created|followersCount|friendsCount|statusesCount|     favouritesCount|listedCount|mediaCount|            location|           protected|             linkUrl|     profileImageUrl|    profileBannerUrl|          profileUrl|\n",
      "+--------------------+------------------+--------+--------------------+---------------+--------+--------------------+--------------+------------+-------------+--------------------+-----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      ShashiRajbhar6|    Shashi Rajbhar|1.02e+18|Satya presan 🤔ho...|           null|    null|                null|          null|        null|         null|                null|       null|      null|                null|                null|                null|                null|                null|                null|\n",
      "|jhuth se samjhaut...|              null|    null|                null|           null|    null|                null|          null|        null|         null|                null|       null|      null|                null|                null|                null|                null|                null|                null|\n",
      "|Jai Shree Ram 🕉 ...|                []|   False|2018-07-08T14:44:...|           1788|    1576|               14396|         26071|           1|          254|Azm Uttar Pradesh...|      False|      null|https://pbs.twimg...|https://pbs.twimg...|https://twitter.c...|                null|                null|                null|\n",
      "|     kaursuk06272818|KAUR SUKH🌾ਕੌਰ ਸੁਖ|1.33e+18|ਜਿਓਣਾ ਕੀ ਸਰੀਰਾਂ ਦ...|             []|   False|2020-11-29T06:40:...|            51|          68|         1338|                3676|          0|       607|                null|               False|                null|https://pbs.twimg...|https://pbs.twimg...|https://twitter.c...|\n",
      "|       SukhdevSingh_|     Sukhdev Singh|1.31e+18|Just a part of my...|             []|   False|2020-09-22T10:45:...|          2595|        3314|         3281|                3533|          0|       519|       Punjab, India|               False|                null|https://pbs.twimg...|https://pbs.twimg...|https://twitter.c...|\n",
      "+--------------------+------------------+--------+--------------------+---------------+--------+--------------------+--------------+------------+-------------+--------------------+-----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now read in data from hdfs\n",
    "\n",
    "\n",
    "tweets = spark.read.option(\"header\",\"true\").option(\"delimiter\", \",\").csv('/tweets.csv')\n",
    "users = spark.read.option(\"header\",\"true\").option(\"delimiter\", \",\").csv('/users.csv')\n",
    "\n",
    "\n",
    "tweets.show(5)\n",
    "users.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe6974c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tweetUrl: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- renderedContent: string (nullable = true)\n",
      " |-- tweetId: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- replyCount: string (nullable = true)\n",
      " |-- retweetCount: string (nullable = true)\n",
      " |-- likeCount: string (nullable = true)\n",
      " |-- quoteCount: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- media: string (nullable = true)\n",
      " |-- retweetedTweet: string (nullable = true)\n",
      " |-- quotedTweet: string (nullable = true)\n",
      " |-- mentionedUsers: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3da3d370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- username: string (nullable = true)\n",
      " |-- displayname: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- rawDescription: string (nullable = true)\n",
      " |-- descriptionUrls: string (nullable = true)\n",
      " |-- verified: string (nullable = true)\n",
      " |-- created: string (nullable = true)\n",
      " |-- followersCount: string (nullable = true)\n",
      " |-- friendsCount: string (nullable = true)\n",
      " |-- statusesCount: string (nullable = true)\n",
      " |-- favouritesCount: string (nullable = true)\n",
      " |-- listedCount: string (nullable = true)\n",
      " |-- mediaCount: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- protected: string (nullable = true)\n",
      " |-- linkUrl: string (nullable = true)\n",
      " |-- profileImageUrl: string (nullable = true)\n",
      " |-- profileBannerUrl: string (nullable = true)\n",
      " |-- profileUrl: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa0f3c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:====================================================>    (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets DF Rows: 3113384\n",
      " Tweets DF Columns: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "row_count = tweets.count()\n",
    "column_count = len(tweets.columns)\n",
    "\n",
    "print(\"Tweets DF Rows: {}\".format(row_count))\n",
    "print(\" Tweets DF Columns: {}\".format(column_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20b68387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users DF Rows: 345992\n",
      "Users DF Columns: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "row_count2 = users.count()\n",
    "column_count2 = len(users.columns)\n",
    "\n",
    "print(\"Users DF Rows: {}\".format(row_count2))\n",
    "print(\"Users DF Columns: {}\".format(column_count2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8021311c",
   "metadata": {},
   "source": [
    "Now lets check for issues where the hastag isnt present in tweets and create new dataframe with the ones that do contain FarmersProtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17b12aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag = 'FarmersProtest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d31efb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "correct_tweets = tweets.filter(F.lower(tweets.renderedContent).rlike(fr\"(?i){hashtag}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b123b8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count = correct_tweets .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72d308a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets with the hashtag FarmersProtest : 439004\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tweets with the hashtag\", hashtag, \":\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd84e1e8",
   "metadata": {},
   "source": [
    "NOw lets merge the dataframes, keeping userId as the join\n",
    "\n",
    "Pandas uses merge, but Spark uses a 'join' method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8e3b575",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = correct_tweets.join(users , on='userID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a74bb439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:====================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+------------+---------+----------+--------------------+--------------------+--------------+-----------+--------------------+--------------+-------------+--------------------+---------------+--------+--------------------+--------------+------------+-------------+---------------+-----------+----------+----------------+---------+-------+--------------------+--------------------+--------------------+\n",
      "|              userId|            tweetUrl|                date|     renderedContent|             tweetId|replyCount|retweetCount|likeCount|quoteCount|              source|               media|retweetedTweet|quotedTweet|      mentionedUsers|      username|  displayname|      rawDescription|descriptionUrls|verified|             created|followersCount|friendsCount|statusesCount|favouritesCount|listedCount|mediaCount|        location|protected|linkUrl|     profileImageUrl|    profileBannerUrl|          profileUrl|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+------------+---------+----------+--------------------+--------------------+--------------+-----------+--------------------+--------------+-------------+--------------------+---------------+--------+--------------------+--------------+------------+-------------+---------------+-----------+----------+----------------+---------+-------+--------------------+--------------------+--------------------+\n",
      "|1.160494097708138...|https://twitter.c...|2021-06-14 05:21:...|@noconversion It ...|1.404307984688173...|         0|           0|        0|         0|\"<a href=\"\"http:/...|                null|          null|       null|[{'_type': 'snscr...|     Kush_2308|Kushagra 🇮🇳|Lawyer| Nationali...|           null|    null|                null|          null|        null|         null|           null|       null|      null|            null|     null|   null|                null|                null|                null|\n",
      "|1.160494097708138...|https://twitter.c...|2021-06-04 14:07:...|@Shivam_h9 This w...|1.400816595186819...|         0|           0|        0|         0|\"<a href=\"\"http:/...|                null|          null|       null|[{'_type': 'snscr...|     Kush_2308|Kushagra 🇮🇳|Lawyer| Nationali...|           null|    null|                null|          null|        null|         null|           null|       null|      null|            null|     null|   null|                null|                null|                null|\n",
      "|1.259170348920168...|https://twitter.c...|2021-02-03 18:21:...|We love our farme...|1.357031378164961...|         4|          62|       95|         4|\"<a href=\"\"http:/...|[{'previewUrl': '...|          null|       null|                null|Mohit__yadav19|  Mohit Yadav|       (traveller 🚙|           null|   False|2020-05-09T17:16:...|           145|         500|         5817|          35599|          0|       311|New Delhi, India|    False|   null|https://pbs.twimg...|https://pbs.twimg...|https://twitter.c...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+------------+---------+----------+--------------------+--------------------+--------------+-----------+--------------------+--------------+-------------+--------------------+---------------+--------+--------------------+--------------+------------+-------------+---------------+-----------+----------+----------------+---------+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3012dd",
   "metadata": {},
   "source": [
    "Sccesfully joined both datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d66ee8b",
   "metadata": {},
   "source": [
    "Finally for now, lets justt keep columns of interest to us (renderedContent, date and username) for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23ffe242",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = [\"date\",\"username\",\"renderedContent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85aee205",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = test_df.select(keep_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d1e1451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:====================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+\n",
      "|                date|       username|     renderedContent|\n",
      "+--------------------+---------------+--------------------+\n",
      "|2021-06-14 05:21:...|      Kush_2308|@noconversion It ...|\n",
      "|2021-06-04 14:07:...|      Kush_2308|@Shivam_h9 This w...|\n",
      "|2021-02-03 18:21:...| Mohit__yadav19|We love our farme...|\n",
      "|2021-02-02 21:02:...| Mohit__yadav19|Thanks for standi...|\n",
      "|2021-02-02 20:56:...| Mohit__yadav19|whole Bollywood i...|\n",
      "|2021-02-02 20:47:...| Mohit__yadav19|The whole world i...|\n",
      "|2021-02-02 20:45:...| Mohit__yadav19|You bjp agent how...|\n",
      "|2021-02-02 20:32:...| Mohit__yadav19|International peo...|\n",
      "|2021-11-19 06:15:...| Mohit__yadav19|Today I remembere...|\n",
      "|2021-11-19 05:40:...|    Amritsohana|Finally We Won🙏❤...|\n",
      "|2021-04-16 03:39:...|  curioussinghs|@pacifistrebel Ab...|\n",
      "|2021-10-31 13:22:...|  curioussinghs|#FarmersProtest h...|\n",
      "|2021-02-05 17:27:...|  DrGauravGarg4|#UNHRC issues adv...|\n",
      "|2021-08-29 05:53:...|Rajwant79905048|I think the bigge...|\n",
      "|2021-11-19 12:36:...|Rajwant79905048|❤️ #FarmLaws #Far...|\n",
      "|2021-03-29 13:28:...|ManvirK88547363|Doing my bit by r...|\n",
      "|2021-02-10 07:37:...|ManvirK88547363|@lilyungpapi Agre...|\n",
      "|2021-02-08 08:43:...|ManvirK88547363|Nation needs prot...|\n",
      "|2021-02-07 17:38:...|ManvirK88547363|@ravneetmansa @di...|\n",
      "|2021-02-03 08:37:...|ManvirK88547363|@Tractor2twitr We...|\n",
      "+--------------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2532f87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:====================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Spark Work DF Rows: 818753\n",
      "Final Spark Work DF Columns: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "row_count3 = final_df.count()\n",
    "column_count3 = len(final_df.columns)\n",
    "\n",
    "print(\"Final Spark Work DF Rows: {}\".format(row_count3))\n",
    "print(\"Final Spark Work DF Columns: {}\".format(column_count3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9837e7ba",
   "metadata": {},
   "source": [
    "## 1.4 MongoDB Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad84f07",
   "metadata": {},
   "source": [
    "Here the plan is to store the above dataframe into the NoSQL DB - MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef84f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pymongo==3.11.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b794a83",
   "metadata": {},
   "source": [
    "I got stuck with recurrent errors here, ended up having to downgrade versions of Pymongo to 3.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6af04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7eade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = MongoClient('mongodb://localhost:27017')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169472ce",
   "metadata": {},
   "source": [
    "Here I needed to download the mogodb spark connector:\n",
    "    \n",
    "    \n",
    "This was downlaoded from : \n",
    "\n",
    "https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/3.0.1/\n",
    "    \n",
    "THen moved to spark jars directory with:\n",
    "\n",
    " cp /home/hduser/Downloads/mongo-spark-connector_2.12-3.0.1.jar /usr/local/spark-3.1.3-bin-hadoop3.2/jars/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49c03512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec38d4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.3\n"
     ]
    }
   ],
   "source": [
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4da4f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8dfbfa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'my_collection' created successfully.\n",
      "Inserted 818753 documents into 'my_collection'.\n"
     ]
    }
   ],
   "source": [
    "# Convert the DataFrame to a list of dictionaries\n",
    "data = final_df.toJSON().map(lambda x: eval(x)).collect()\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "db = client['local']\n",
    "\n",
    "# Create a new collection and insert the data\n",
    "collection_name = 'my_collection'\n",
    "db[collection_name].insert_many(data)\n",
    "\n",
    "# Verify the collection and inserted documents\n",
    "collection_names = db.list_collection_names()\n",
    "if collection_name in collection_names:\n",
    "    print(f\"Collection '{collection_name}' created successfully.\")\n",
    "    print(f\"Inserted {len(data)} documents into '{collection_name}'.\")\n",
    "else:\n",
    "    print(f\"Failed to create collection '{collection_name}'.\")\n",
    "\n",
    "# Close the MongoDB connection\n",
    "client.close()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c0635",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
